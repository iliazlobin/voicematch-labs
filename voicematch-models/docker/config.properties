# vmargs=-XX:+UseContainerSupport -XX:InitialRAMPercentage=8.0 -XX:MaxRAMPercentage=10.0 -XX:-UseLargePages -XX:+UseG1GC -XX:+ExitOnOutOfMemoryError -Dlog4j.configurationFile=file:///home/model-server/config.properties/log4j2.xml
vmargs=-XX:+UseContainerSupport -XX:InitialRAMPercentage=8.0 -XX:MaxRAMPercentage=10.0 -XX:-UseLargePages -XX:+UseG1GC -XX:+ExitOnOutOfMemoryError
model_store=/opt/ml/model
load_models=ALL
inference_address=http://0.0.0.0:7070
management_address=http://0.0.0.0:7071
enable_envvars_config=true
# management_address=unix:/tmp/management.sock
# number_of_netty_threads=0
# netty_client_threads=0
# default_response_timeout=120
# default_workers_per_model=0
# job_queue_size=100
# async_logging=false
# number_of_gpu=1
# cors_allowed_origin
# cors_allowed_methods
# cors_allowed_headers
# keystore=src/test/resources/keystore.p12
# keystore_pass=changeit
# keystore_type=PKCS12
# private_key_file=src/test/resources/key.pem
# certificate_file=src/test/resources/certs.pem
# max_response_size=6553500
# max_request_size=6553500
# blacklist_env_vars=
# decode_input_request=false
enable_metrics_api=false

# Based on https://github.com/pytorch/serve/blob/master/docs/configuration.md
enable_envvars_config=true
decode_input_request=false
load_models=ALL
default_response_timeout=60
inference_address=http://0.0.0.0:7070
management_address=http://0.0.0.0:7071
default_service_handler=/opt/conda/lib/python3.9/site-packages/sagemaker_pytorch_serving_container/handler_service.py:handle
