{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802603125572205,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
       "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
       "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
       "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
       "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
       "         -7.8940, -6.7200, -4.6759, -6.3278, -4.8339, -5.1839, -3.3724, -7.4120,\n",
       "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
       "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
       "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.3958e+00, -7.0978e+00, -7.0745e+00, -6.3676e+00, -5.9532e+00,\n",
       "         -7.9585e+00, -7.1869e+00, -3.6494e+00, -6.9677e+00, -5.1421e+00,\n",
       "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
       "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6687e-01, -3.9180e+00,\n",
       "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5442e-03,\n",
       "         -1.5531e+00, -6.9469e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
       "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
       "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
       "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
       "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
       "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
       "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
       "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
       "         -1.8462e+00, -5.1420e+00]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c65e9e796324f01ccc2de1a4ebdf90e4cfd000cc4b64bd62190b1ef74c9ba484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
